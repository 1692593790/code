{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a185c044",
   "metadata": {},
   "source": [
    "# 1.数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6726b287",
   "metadata": {},
   "outputs": [],
   "source": [
    "#假设已从LS-DYNA仿真中获得了数据并存为了CSV文件\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "\n",
    "#导入应变 FRP弹模 混凝土抗压强度 FRP厚度 FRP极限强度的数据\n",
    "data = pd.read_csv(\"文件路径\")\n",
    "\n",
    "#定义时序参数\n",
    "TIME_STEPS = 30  # 输入时间步长\n",
    "PRED_STEPS = 3   # 预测步长\n",
    "\n",
    "#选择需要的特征 假设是轴向应变 \n",
    "features = data[['strain', 'elastic_modulus_FRP','compressive_strength_concrete', 'FRP_thickness', 'ultimate_strength_FRP' ]].values\n",
    "\n",
    "#构建时间窗口数据集\n",
    "def create_sequences(data, time_steps, pred_steps):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - time_steps - pred_steps + 1):\n",
    "        # 输入：连续30个时间步的所有特征\n",
    "        X.append(data[i:i+time_steps, :])  \n",
    "        # 输出：后续3个时间步的应变值（假设strain是第一列）\n",
    "        y.append(data[i+time_steps:i+time_steps+pred_steps, 0])  \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X, y = create_sequences(features, TIME_STEPS, PRED_STEPS)\n",
    "\n",
    "#数据标准化（归一化）\n",
    "scaler_x = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "\n",
    "#将三维数据重塑为二维进行标准化\n",
    "original_shape = X.shape\n",
    "X = scaler_x.fit_transform(X.reshape(-1, X.shape[-1])).reshape(original_shape)\n",
    "y = scaler_y.fit_transform(y.reshape(-1, 1)).reshape(y.shape[0], -1)\n",
    "\n",
    "#划分训练集和测试集\n",
    "train_size = int(0.8 * len(X))\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "#转换为torch张量\n",
    "#自定义数据集类\n",
    "class StrainDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = torch.tensor(features,dtype = torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype = torch.float32)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        return self.features[idx], self.labels[idx]\n",
    "    \n",
    "#构建数据加载器\n",
    "train_dataset = StrainDataset(X_train, y_train)\n",
    "test_dataset = StrainDataset(X_test, y_test)\n",
    "#假设batch_size均为128\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373433fd",
   "metadata": {},
   "source": [
    "# 2.构建模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88451ea8",
   "metadata": {},
   "source": [
    "## 2.1 LSTM网络"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37caffc",
   "metadata": {},
   "source": [
    "### 2.1.1 LSTM网络的搭建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74215680",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "#原文给出的是LSTM模型是具有128个神经元的单层结构  有两个具有64个和3个神经元的Dense层\n",
    "#但由于文中给出的是GRU的原理图 因此按照原文进行建模\n",
    "class StrainPredictionLSTM(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,output_size,num_layers):\n",
    "        super( StrainPredictionLSTM,self).__init__()\n",
    "        \n",
    "        #LSTM层 \n",
    "        self.lstm = nn.LSTM(input_size,hidden_size,num_layers,batch_first=True)\n",
    "        \n",
    "        #全连接层，连接LSTM输出到最终预测 假设激活函数用的是RELU\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size,64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,output_size)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #LSTM层的前向传播 假设没有定义要在GPU上跑\n",
    "        lstm_out,_ = self.lstm(x) # lstm_out包含所有时间步的输出\n",
    "        \n",
    "        #取最后一个时间步的输出作为预测\n",
    "        last_step = lstm_out[:,-1,:]\n",
    "        \n",
    "        return self.fc(last_step) \n",
    "\n",
    "#假设输入数据的维度位：batch_size×seq_len×input_size\n",
    "#输入维度位30个时间步 特征维度为1\n",
    "input_size = 5  # 输入特征的维度\n",
    "hidden_size = 128  # LSTM隐藏层的维度\n",
    "output_size = 3 # 输出维度\n",
    "num_layers = 1  # LSTM层数\n",
    "seq_len = 30  # 序列长度\n",
    "\n",
    "#创建模型\n",
    "model_LSTM = StrainPredictionLSTM(input_size = input_size, hidden_size = hidden_size, output_size = output_size, num_layers = num_layers)\n",
    "    \n",
    "#打印模型\n",
    "print(model_LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39175f51",
   "metadata": {},
   "source": [
    "### 2.1.1 LSTM网络的训练和评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662344d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from torchmetrics import MeanAbsolutePercentageError\n",
    "\n",
    "#定义损失函数和优化器\n",
    "criterion_LSTM_rmse = nn.MSELoss()\n",
    "criterion_LSTM_mape = MeanAbsolutePercentageError()\n",
    "\n",
    "\n",
    "#训练过程\n",
    "def train(model_LSTM, train_loader,test_loader, optimizer, num_epochs=400):#假设均进行400epoch\n",
    "    \n",
    "    train_rmse, test_rmse = [], []\n",
    "    for epoch in range(num_epochs):\n",
    "        #训练阶段\n",
    "        model_LSTM.train()\n",
    "        running_lstm_rmse_loss = 0.0\n",
    "        #running_lstm_mape_loss = 0.0\n",
    "        for inputs,targets in train_loader:\n",
    "            optimizer_LSTM.zero_grad()\n",
    "            #前向传播\n",
    "            outputs = model_LSTM(inputs)\n",
    "            #计算损失 以计算rmse为例\n",
    "            lstm_rmse_loss = criterion_LSTM_rmse(outputs, targets)\n",
    "            #lstm_mape_loss = criterion_LSTM_mape(outputs, targets)\n",
    "            #反向传播\n",
    "            lstm_rmse_loss.backward()\n",
    "           # lstm_mape_loss.backward()\n",
    "            #更新参数\n",
    "            optimizer.step()\n",
    "            running_lstm_rmse_loss += lstm_rmse_loss.item()\n",
    "            #running_lstm_mape_loss += lstm_mape_loss.item()\n",
    "            \n",
    "        #评估阶段\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            #训练集评估\n",
    "            train_preds = torch.cat([model(batch[0]) for batch in train_loader])\n",
    "            train_rmse.append(torch.sqrt(criterion(train_preds, train_loader.dataset.labels)).item())\n",
    "            \n",
    "            #测试集评估\n",
    "            test_preds = torch.cat([model(batch[0]) for batch in test_loader])\n",
    "            test_rmse.append(torch.sqrt(criterion(test_preds, test_loader.dataset.labels)).item())\n",
    "            \n",
    "       print(f'Epoch {epoch+1}/{num_epochs}, '\n",
    "              f'Train RMSE: {train_rmse:.4f}, '\n",
    "              f'Test RMSE: {test_rmse:.4f}')\n",
    "    \n",
    "    return train_rmse, test_rmse  # 返回历史记录     \n",
    "\n",
    "        \n",
    "# 模型初始化（注意输入维度修正）\n",
    "model_LSTM = StrainPredictionLSTM(input_size=5)  # 关键参数修正\n",
    "\n",
    "# 优化器设置\n",
    "optimizer = optim.Adam(model_LSTM.parameters(), lr=1e-4)  \n",
    "\n",
    "# 启动训练\n",
    "train_rmse, test_rmse = train(\n",
    "    model=model_LSTM,\n",
    "    train_loader=train_loader,\n",
    "    test_loader=test_loader,\n",
    "    optimizer=optimizer\n",
    ")\n",
    "\n",
    "#计算最终的MAPE  #可以将损失函数改为MAPE再算一次  因为没办法同时返回两个损失函数\n",
    "#但是可以把他们组合一下 即每个损失函数乘以一定的权重相加\n",
    "with torch.no_grad():\n",
    "    y_true = test_loader.dataset.labels\n",
    "    y_pred = model_LSTM(test_loader.dataset.features)\n",
    "    mape = criterion_LSTM_mape(y_pred, y_true).item() * 100 \n",
    "    \n",
    "print(f'Final Test RMSE: {test_rmse[-1]:.4f}')\n",
    "print(f'Final Test MAPE: {mape:.2f}%')\n",
    "\n",
    "#还需要绘制RMSE MAPE strain 的曲线  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bff5a3f",
   "metadata": {},
   "source": [
    "## 2.2 GRU网络"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7093dd",
   "metadata": {},
   "source": [
    "### 2.2.1 GRU网络的搭建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86228228",
   "metadata": {},
   "outputs": [],
   "source": [
    "#这里重新引入部分库\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class StrainPredictionGRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size, num_layers=2):\n",
    "        super(StrainPredictionGRU, self).__init__()\n",
    "        \n",
    "        #定义GRU层\n",
    "        self.gru = nn.GRU(input_size, hidden_size1, num_layers, batch_first=True)\n",
    "        \n",
    "        #定义第二层GRU\n",
    "        self.gru2 = nn.GRU(hidden_size1, hidden_size2, num_layers, batch_first=True)\n",
    "        \n",
    "        #定义全连接层\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size2, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32)\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, output_size)\n",
    "        )\n",
    "        \n",
    "        def forward(self,x):\n",
    "            #第一个GRU层的输出\n",
    "            gru_out,_ = self.gru(x)\n",
    "            \n",
    "            #第二个GRU层的输出\n",
    "            gru_out2,_ = self.gru2(gru_out)\n",
    "            \n",
    "            #使用最后一个时间步的隐藏状态去预测\n",
    "            last_hidden_state = gru_out2[:,-1,:]\n",
    "            \n",
    "            #通过全连接层进行处理\n",
    "            x = self.fc1(last_hidden_state)\n",
    "            #x = self.dropout(x)  # Dropout层\n",
    "            x = self.fc2(x)\n",
    "            x = self.fc3(x)  # 最终输出层，3个预测值\n",
    "            \n",
    "            return x \n",
    "        \n",
    "#模型参数\n",
    "input_size = 5  # 每个时间步的特征数\n",
    "hidden_size1 = 256  # 第一个GRU层的隐藏层大小\n",
    "hidden_size2 = 128  # 第二个GRU层的隐藏层大小\n",
    "output_size = 3  # 预测未来3个时间步的应变值\n",
    "num_layers = 2  # GRU层数\n",
    "seq_len = 30  # 输入序列长度（30个时间步）\n",
    "\n",
    "#创建模型\n",
    "model_GRU = StrainPredictionGRU(input_size=input_size, hidden_size1=hidden_size1, hidden_size2=hidden_size2, output_size=output_size, num_layers=num_layers)\n",
    "\n",
    "#打印模型结构\n",
    "print(model_GRU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f2bca4",
   "metadata": {},
   "source": [
    "### 2.2.2 GRU网络的训练与评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9856f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset,DataLoader\n",
    "import numpy as np\n",
    "\n",
    "#之前已经定义过了数据集 这里直接定义损失函数rmse和优化器\n",
    "criterion_GRU_rmse = np.sqrt(mean_squared_error(all_labels, all_preds))\n",
    "optimizer_GRU = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "#训练过程\n",
    "def train(model_GRU, train_loader, test_loader, num_epochs=400):\n",
    "    train_rmse_list, test_rmse_list = [], []\n",
    "    for epoch in range(num_epochs):\n",
    "        #训练阶段：\n",
    "        model_GRU.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model_GRU(inputs)\n",
    "            loss = criterion_GRU_rmse(outputs,targets)\n",
    "            loss.backward()\n",
    "            optimizer_GRU.step()\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "        #评估阶段\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # 训练集评估\n",
    "            train_preds = torch.cat([model(batch[0]) for batch in train_loader])\n",
    "            train_rmse = torch.sqrt(criterion(train_preds, train_loader.dataset.labels)).item()\n",
    "            \n",
    "            # 测试集评估\n",
    "            test_preds = torch.cat([model(batch[0]) for batch in test_loader])\n",
    "            test_rmse = torch.sqrt(criterion(test_preds, test_loader.dataset.labels)).item()\n",
    "        \n",
    "        train_rmse_list.append(train_rmse)\n",
    "        test_rmse_list.append(test_rmse)\n",
    "        print(f'Epoch {epoch+1}/{num_epochs} | '\n",
    "              f'Train RMSE: {train_rmse:.4f} | '\n",
    "              f'Test RMSE: {test_rmse:.4f}')\n",
    "            \n",
    "       \n",
    "    return train_rmse_list, test_rmse_list\n",
    "        \n",
    "\n",
    "# 模型初始化\n",
    "model_GRU = StrainPredictionGRU(input_size=5, hidden_size1=256, hidden_size2 = 128)\n",
    "\n",
    "# 优化器设置（按论文参数）\n",
    "optimizer_GRU = optim.Adam(model_GRU.parameters(), lr=1e-4)\n",
    "\n",
    "# 启动训练\n",
    "history = train(\n",
    "    model=model_GRU,\n",
    "    train_loader=train_loader,\n",
    "    test_loader=test_loader,\n",
    "    optimizer=optimizer\n",
    ")\n",
    "\n",
    "# 最终评估\n",
    "def final_evaluate(model, loader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_true = loader.dataset.labels\n",
    "        y_pred = model(loader.dataset.features)\n",
    "        rmse = torch.sqrt(nn.MSELoss()(y_pred, y_true)).item()\n",
    "        mape = (torch.abs((y_pred - y_true)/y_true).mean().item() * 100\n",
    "    return rmse, mape\n",
    "\n",
    "train_rmse, train_mape = final_evaluate(model_GRU, train_loader)\n",
    "test_rmse, test_mape = final_evaluate(model_GRU, test_loader)\n",
    "\n",
    "print(f'[Final] Train RMSE: {train_rmse:.4f}, MAPE: {train_mape:.2f}%')\n",
    "print(f'[Final] Test RMSE: {test_rmse:.4f}, MAPE: {test_mape:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd71fbe8",
   "metadata": {},
   "source": [
    "## 2.3 搭建Basic GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578b53fe",
   "metadata": {},
   "source": [
    "### 2.3.1 搭建生成器GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12e903a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_features=5, output_features=1, seq_len=30):\n",
    "        super(Generator, self).__init__()\n",
    "        self.gru1 = nn.GRU(input_features, 1024, batch_first = True)\n",
    "        self.gru2 = nn.GRU(1024, 512, batch_first = True)\n",
    "        self.gru3 = nn.GRU(512, 256, batch_first = True)\n",
    "        self.fc1 = nn.Linear(256,128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, output_features)\n",
    "        \n",
    "    def forward(self, z):\n",
    "        out,_ = self.gru1(z)\n",
    "        out1,_ = self.gru2(out)\n",
    "        out2,_ = self.gru3(out1)\n",
    "    \n",
    "    #全连接层 使用最后一个时间步的隐藏状态进行预测\n",
    "        out3 = torch.relu(self.fc1(out2[:,-1,:]))\n",
    "        out4 = self.fc2(out3)\n",
    "        out5 = self.fc3(out4)\n",
    "        \n",
    "        out5 = out5.unsqueeze(1).repeat(1, 3, 1)\n",
    "    \n",
    "        return out5\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33091d26",
   "metadata": {},
   "source": [
    "### 2.3.2 搭建判别器CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a27e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#判别器通常倾向于判断整个序列的真假 而不是序列中某个值的真假\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_channels):\n",
    "        super(Discriminator, self).__init__()\n",
    "        #conv1d用于处理一维数据（时间序列 文本数据 音频）\n",
    "        self.conv1 = nn.Conv1d(input_channels=1,32,3)\n",
    "        self.conv2 = nn.Conv1d(32,64,3)\n",
    "        self.conv3 = nn.Conv1d(64, 128, 3)\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc1 = nn.Linear(128, 220)\n",
    "        self.fc2 = nn.Linear(220,1)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        # 将 (batch_size, seq_len, input_channels) 转换为 (batch_size, input_channels, seq_len)\n",
    "        x = x.permute(0, 2, 1)  # 交换维度，以适应Conv1d输入\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.relu(self.conv3(x))\n",
    "        # 自适应池化至固定大小，并移除最后一个维度\n",
    "        x = self.pool(x).squeeze(-1)\n",
    "        #全连接层\n",
    "        x = torch.LeakyReLU(self.fc1(x),negative_slope=0.01)\n",
    "        #输出一个值，表示整个序列的真假\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        \n",
    "        return x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544e477d",
   "metadata": {},
   "source": [
    "### 2.3.3 GAN的搭建与训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d29977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 损失函数：RMSE\n",
    "def criterion_GAN_rmse(real_data, fake_data):\n",
    "    mse_loss = nn.MSELoss()\n",
    "    return torch.sqrt(mse_loss(real_data, fake_data))\n",
    "\n",
    "class GAN(nn.Module):\n",
    "    def __init__(self, generator, discriminator):\n",
    "        super(GAN, self).__init__()\n",
    "        self.genarator = generator\n",
    "        self.discriminator = discriminator\n",
    "      #因为是时间序列问题，因此不需要随机噪声  \n",
    "    def forward(self, real_data):\n",
    "        fake_data = self.generator(real_data)\n",
    "        return fake_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdc8e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#创建生成器和判别器\n",
    "generator = Generator(input_features=5)\n",
    "discriminator = Discriminator(input_channels=1)\n",
    "\n",
    "# 优化器\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=1e-4)\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=1e-4)\n",
    "\n",
    "#计算RMSE和MAPE\n",
    "def evaluate_model_GAN(y_true, y_pred):\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))  # 计算RMSE\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100  # 计算MAPE\n",
    "    return rmse, mape\n",
    "\n",
    "#训练函数\n",
    "def train_GAN(generator, discriminator, epochs, batch_size=128,seq_len=30):\n",
    "    for epoch in range(epochs):\n",
    "        # 训练判别器\n",
    "        for _ in range(5):  # 判别器更新次数\n",
    "        \n",
    "            z =  X_train_tensor[:batch_size, :seq_len, :]   # 获取真实数据\n",
    "            fake_data = generator(z)  # 通过生成器生成假数据\n",
    "\n",
    "            # 获取真实数据，时间步 31 到 33\n",
    "            real_data = X_train_tensor[batch_size, seq_len:seq_len+3, :]  # 真实数据，时间步 31 到 33\n",
    "            \n",
    "            # 判别器优化\n",
    "            optimizer_D.zero_grad()\n",
    "            real_labels = torch.ones(batch_size, 1)  # 真实标签\n",
    "            fake_labels = torch.zeros(batch_size, 1)  # 假标签\n",
    "            \n",
    "            real_loss = criterion_GAN_rmse(real_data, real_labels)\n",
    "            fake_loss = criterion_GAN_rmse(fake_data, fake_labels)\n",
    "            d_loss = (real_loss + fake_loss) / 2\n",
    "\n",
    "            d_loss.backward()\n",
    "            optimizer_D.step()\n",
    "            \n",
    "            \n",
    "         # 训练生成器\n",
    "        z =  X_train_tensor[:batch_size, :seq_len, :] \n",
    "        fake_data = generator(z)\n",
    "\n",
    "        # 生成器优化\n",
    "        optimizer_G.zero_grad()\n",
    "        g_loss = -torch.mean(discriminator(fake_data))\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        print(f\"Epoch {epoch}/{epochs} [D loss: {d_loss.item()}] [G loss: {g_loss.item()}]\")\n",
    "        \n",
    "         # 在每个epoch结束后，评估模型性能（例如在训练集上计算RMSE和MAPE）\n",
    "        if epoch % 10 == 0:  # 每10个epoch进行一次评估\n",
    "            y_pred = fake_data.detach().numpy()  # 获取生成的假数据并转化为numpy数组\n",
    "            y_true = y_train_tensor[batch_size, seq_len:seq_len+3, :].detach().numpy()  # 获取真实数据（31-33时间步的数据）\n",
    "            \n",
    "            # 计算 RMSE 和 MAPE\n",
    "            rmse, mape = evaluate_model_GAN(y_true, y_pred)\n",
    "            print(f\"Epoch {epoch} [RMSE: {rmse}] [MAPE: {mape}]\")\n",
    "\n",
    "# 训练模型\n",
    "train_GAN(generator, discriminator, epochs=400, batch_size=128)\n",
    "\n",
    "\n",
    "# 在测试集上评估模型\n",
    "def evaluate_on_test(generator, X_test_tensor, y_test_tensor):\n",
    "    # 使用生成器生成预测数据\n",
    "    y_pred = generator(X_test_tensor)  # 生成器的输出（假数据）\n",
    "\n",
    "    # 计算评估指标\n",
    "    rmse, mape = evaluate_model_GAN(y_test_tensor.numpy(), y_pred.detach().numpy())  # 注意转换为numpy数组\n",
    "    print(f\"RMSE: {rmse}, MAPE: {mape}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2e6bca",
   "metadata": {},
   "source": [
    "## 2.4 StrainNet  即WGAN-GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c238d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN和GAN中的discriminator相同  generator不同  因此重新设置GRU网络\n",
    "class StrainGenerator(nn.Module):\n",
    "    def __init__(self, input_features=5, output_features=1, seq_len=30):\n",
    "        super(Generator, self).__init__()\n",
    "        self.gru1 = nn.GRU(input_features, 256, batch_first=True)\n",
    "        self.gru2 = nn.GRU(256, 128, batch_first=True)\n",
    "        self.fc1 = nn.Linear(128, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)  \n",
    "        self.fc3 = nn.Linear(32, output_features)\n",
    "\n",
    "    def forward(self, z):\n",
    "        out, _ = self.gru1(z)\n",
    "        out, _ = self.gru2(out)\n",
    "        out = torch.relu(self.fc1(out[:, -1, :]))  # 取最后一个时间步的输出\n",
    "        out = self.fc2(out)\n",
    "        out = self.fc3(out)\n",
    "        out =out.unsqueeze(1).repeat(1,3, 1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34690b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StrainNetTrainer:\n",
    "    def __init__(self, latent_dim=5, gp_weight=10):\n",
    "        self.generator = StrainGenerator(latent_dim)\n",
    "        self.discriminator = Discriminator()\n",
    "        self.gp_weight = gp_weight\n",
    "        \n",
    "        # 使用RMSprop优化器\n",
    "        self.opt_g = optim.RMSprop(self.generator.parameters(), lr=1e-4)\n",
    "        self.opt_d = optim.RMSprop(self.discriminator.parameters(), lr=1e-4)\n",
    "        \n",
    "    def gradient_penalty(self, real, fake):\n",
    "        batch_size = real.size(0)\n",
    "         # 生成一个 alpha 向量，alpha 是一个在 [0, 1] 区间均匀分布的随机数\n",
    "        # alpha 用来生成真实数据和假数据之间的插值\n",
    "        alpha = torch.rand(batch_size, 1, 1)\n",
    "        # 计算生成的插值数据：线性插值\n",
    "        interpolates = (alpha * real + (1 - alpha) * fake).requires_grad_(True)\n",
    "        # 通过判别器对插值数据进行评分\n",
    "        d_interpolates = self.discriminator(interpolates)\n",
    "        \n",
    "        # 计算梯度：我们通过自动微分计算判别器对插值数据的梯度\n",
    "        gradients = torch.autograd.grad(\n",
    "            outputs=d_interpolates,# 判别器的输出\n",
    "            inputs=interpolates,# 插值数据\n",
    "            grad_outputs=torch.ones_like(d_interpolates),# 梯度的初始值，通常为 1，因为我们计算的是一阶导数\n",
    "            create_graph=True,# 为了计算高阶导数，我们需要保留图\n",
    "            retain_graph=True # 保留计算图用于后续的反向传播\n",
    "        )[0]\n",
    "        \n",
    "        # 展平梯度矩阵，使其变成一个一维的向量，便于计算范数\n",
    "        gradients = gradients.view(batch_size, -1)\n",
    "        # 计算梯度的范数（L2范数），表示梯度的大小\n",
    "        grad_norm = gradients.norm(2, dim=1)\n",
    "        \n",
    "        # 返回梯度惩罚项：计算梯度范数与1的差的平方的均值\n",
    "        return torch.mean((grad_norm - 1) ** 2)\n",
    "    \n",
    "    def train_step(self, real_data):\n",
    "        # 训练判别器\n",
    "        for _ in range(5):  # 判别器迭代次数\n",
    "            # 生成假数据\n",
    "            z =  X_train_tensor[:batch_size, :seq_len, :]\n",
    "            fake_data = self.generator(z)\n",
    "            \n",
    "            # 计算Wasserstein距离\n",
    "            real_loss = -torch.mean(self.discriminator(real_data))\n",
    "            fake_loss = torch.mean(self.discriminator(fake_data.detach()))\n",
    "            gp = self.gradient_penalty(real_data, fake_data)\n",
    "            d_loss = real_loss + fake_loss + self.gp_weight * gp\n",
    "            \n",
    "            self.opt_d.zero_grad()\n",
    "            d_loss.backward()\n",
    "            self.opt_d.step()\n",
    "            \n",
    "    \n",
    "        # 训练生成器\n",
    "        fake_data = self.generator(z)\n",
    "        g_loss = -torch.mean(self.discriminator(fake_data))\n",
    "        \n",
    "        self.opt_g.zero_grad()\n",
    "        g_loss.backward()\n",
    "        self.opt_g.step()\n",
    "        \n",
    "        return {'d_loss': d_loss.item(), 'g_loss': g_loss.item()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784a1c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据加载器示例\n",
    "class StrainDataset(Dataset):\n",
    "    def __init__(self, data_tensor):\n",
    "        # 输入形状应为 (num_samples, 30, 5)\n",
    "        self.data = data_tensor\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "# 初始化数据加载器\n",
    "train_dataset = StrainDataset(X_train_tensor)  # X_train_tensor.shape: (N,30,5)\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cfa035",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_strainnet(generator, train_loader, batch_size=128):\n",
    "    # 生成样本\n",
    "    z =  X_train_tensor[:batch_size, :seq_len, :]\n",
    "    gen_data = generator(z)  # (batch_size, 30, 5)\n",
    "    \n",
    "    # 获取真实样本\n",
    "    real_data = X_train_tensor[batch_size, seq_len:seq_len+3, :]\n",
    "    \n",
    "    # 计算特征统计相似度\n",
    "    metrics_train = {\n",
    "        '特征RMSE': torch.sqrt(((gen_data - real_data[:n_samples])**2).mean()),\n",
    "        '应变分布KL散度': calculate_kl_divergence(\n",
    "            gen_data[:, :, 0].flatten(),  # 应变特征\n",
    "            real_data[:, :, 0].flatten()\n",
    "        )\n",
    "    }\n",
    "    return metrics_train\n",
    "\n",
    "# 在测试集上评估模型\n",
    "def evaluate_on_test_strainnet(generator, X_test_tensor):\n",
    "    z =  X_test_tensor[:batch_size, :seq_len, :]\n",
    "    gen_data = generator(z)  # (batch_size, 30, 5)\n",
    "    \n",
    "    # 获取真实样本\n",
    "    real_data = X_test_tensor[batch_size, seq_len:seq_len+3, :]\n",
    "    \n",
    "    # 计算特征统计相似度\n",
    "    metrics_train = {\n",
    "        '特征RMSE': torch.sqrt(((gen_data - real_data[:n_samples])**2).mean()),\n",
    "        '应变分布KL散度': calculate_kl_divergence(\n",
    "            gen_data[:, :, 0].flatten(),  # 应变特征\n",
    "            real_data[:, :, 0].flatten()\n",
    "        )\n",
    "    }\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b570e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
